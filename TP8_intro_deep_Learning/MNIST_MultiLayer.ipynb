{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "\n",
    "from keras.datasets import mnist\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape each 28x28 image -> 784 dim. vector\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalization\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "from keras.utils import np_utils\n",
    "K=10\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, K)\n",
    "Y_test = np_utils.to_categorical(y_test, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # Input matrix X of size Nbxd - Output matrix of same size\n",
    "    E = np.exp(X)\n",
    "    return (E.T / np.sum(E,axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # Input matrix X of size Nbxd - Output matrix of same size\n",
    "    E = np.exp(-X) + 1\n",
    "    return (1./ E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_sigmoid(Y):\n",
    "    # Input matrix X of size Nbxd - Output matrix of same size\n",
    "    result = np.multiply(Y, 1. - Y)\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnectedLayer:\n",
    "    \n",
    "    def __init__(self, input_dim, nbr_units, activation_funct):\n",
    "        self.input_dim = input_dim\n",
    "        self.nbr_units = nbr_units\n",
    "        self.activation_funct = activation_funct\n",
    "        \n",
    "        #self.W = np.zeros((input_dim, nbr_units))\n",
    "        self.W = np.random.normal(loc=0.0, scale=0.1, size =(input_dim, nbr_units))\n",
    "        #self.W = (1./math.sqrt(20))*np.random.normal(loc=0.0, scale=0.1, size =(input_dim, nbr_units))\n",
    "        #self.b = np.zeros((1, nbr_units))\n",
    "        self.b = np.random.normal(loc=0.0, scale=0.1, size =(1, nbr_units))\n",
    "        #self.b = (1./math.sqrt(20))*np.random.normal(loc=0.0, scale=0.1, size =(1, nbr_units))\n",
    "        \n",
    "        # S = X*W + b\n",
    "        self.S = np.array((0,0))\n",
    "        self.batch_in = np.array((0,0))\n",
    "        self.batch_out = np.array((0,0))\n",
    "        \n",
    "        # dLdS: derivative of Loss with respect to S\n",
    "        self.dLdS = np.array((0,0))\n",
    "        self.gradW = np.array((0,0))\n",
    "        self.gradb = np.array((0,0))\n",
    "\n",
    "    def forward_layer(self, batch_in):\n",
    "        self.batch_in = batch_in\n",
    "        self.S = np.matmul(batch_in,self.W) + self.b\n",
    "        if self.activation_funct == \"sigmoid\":\n",
    "            self.batch_out = sigmoid(self.S)\n",
    "        elif self.activation_funct == \"softmax\":\n",
    "            self.batch_out = softmax(self.S)\n",
    "        else:\n",
    "            print(\"Error: unknown activation function \", act_function)        \n",
    "        return self.batch_out\n",
    "    \n",
    "    # dLdS_next: dLdS: derivative of Loss with respect to S of the adjacent layer\n",
    "    def backward_layer(self, dLdS_next=None, W_next=None, out_expected = None):\n",
    "        batch_size = self.batch_in.shape[0]\n",
    "        if out_expected is None:\n",
    "            self.dLdS = np.matmul(dLdS_next, W_next.T)\n",
    "            delta_batch_out_activ = deriv_sigmoid(self.batch_out)\n",
    "            self.dLdS = np.multiply(self.dLdS, delta_batch_out_activ)          \n",
    "        else:\n",
    "            self.dLdS = self.batch_out - out_expected            \n",
    "            \n",
    "        self.gradW = (1./batch_size)*np.matmul(self.batch_in.T, self.dLdS)\n",
    "        self.gradb = (1./batch_size)*np.sum(self.dLdS, axis=0)\n",
    "        \n",
    "        return self.gradW , self.gradb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = list()\n",
    "        \n",
    "    def addLayer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "     \n",
    "    def forward_net(self, batch_in):\n",
    "        batch_in_dim = batch_in.shape[1]\n",
    "        batch_in_size = batch_in.shape[0]\n",
    "        #Input layer \n",
    "        out = self.layers[0].forward_layer(batch_in)\n",
    "        #The other layers\n",
    "        for i in range(1, len(self.layers)):\n",
    "            out = self.layers[i].forward_layer(out)   \n",
    "        return out\n",
    "        \n",
    "    def backward_net(self, batch_expected): \n",
    "        #Output layer\n",
    "        self.layers[len(self.layers) - 1].backward_layer(out_expected = batch_expected)\n",
    "        #The other layers\n",
    "        for i in reversed(range(0, len(self.layers)-1)):\n",
    "            self.layers[i].backward_layer(dLdS_next=self.layers[i+1].dLdS, W_next=self.layers[i+1].W)\n",
    "\n",
    "    def update_parameters(self, training_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.W = layer.W - training_rate*layer.gradW\n",
    "            layer.b = layer.b - training_rate*layer.gradb\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs, batch_size, training_rate):\n",
    "        N = X_train.shape[0]\n",
    "        nb_batches = int(float(N) / batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            for ex in range(nb_batches):\n",
    "                X_tmp = X_train[ex*batch_size:(ex+1)*batch_size , :]\n",
    "                Y_tmp = Y_train[ex*batch_size:(ex+1)*batch_size]\n",
    "                #Forward pass\n",
    "                self.forward_net(X_tmp)\n",
    "                #Backward pass\n",
    "                self.backward_net(Y_tmp)\n",
    "                #Update parameters\n",
    "                self.update_parameters(training_rate)  \n",
    "    \n",
    "    def accuracy(self, images, labels):\n",
    "        pred = self.forward_net(images)\n",
    "        return np.where( pred.argmax(axis=1) != labels.argmax(axis=1) , 0.,1.).mean()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural net definition \n",
    "\n",
    "N = X_train.shape[0]\n",
    "d = X_train.shape[1]\n",
    "\n",
    "numEp = 100 # Number of epochs for gradient descent\n",
    "eta = 1e-1 # Learning rate\n",
    "batch_size = 100\n",
    "\n",
    "nb_units_l1 = 50\n",
    "nb_units_l2 = 40\n",
    "nb_units_l3 = 10\n",
    "\n",
    "layer1 = FullConnectedLayer(d, nb_units_l1, \"sigmoid\")\n",
    "layer2 = FullConnectedLayer(nb_units_l1, nb_units_l2, \"sigmoid\")\n",
    "layer3 = FullConnectedLayer(nb_units_l2, nb_units_l3, \"softmax\")\n",
    "\n",
    "neuralNet = NeuralNet()\n",
    "neuralNet.addLayer(layer1)\n",
    "neuralNet.addLayer(layer2)\n",
    "neuralNet.addLayer(layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet.train(X_train, Y_train, numEp, batch_size, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralNet.accuracy(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
